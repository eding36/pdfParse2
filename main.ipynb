{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "master_df = pd.read_csv('EPAR_table_4.csv',header = 0,skiprows = 8)\n",
    "df = master_df[master_df['Category'] == 'Human']\n",
    "df2 = df[df['Authorisation status'] == 'Authorised']\n",
    "\n",
    "url_list = df2['URL'].tolist()\n",
    "print(url_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "product_info_url_list = []\n",
    "for i in range(len(url_list)):\n",
    "    success = False\n",
    "    print(i)\n",
    "    while not success:   \n",
    "        link = url_list[i]\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:  \n",
    "            web_content = response.text\n",
    "            soup = BeautifulSoup(web_content, 'html.parser')\n",
    "            soups = soup.find_all('a')\n",
    "            href_links = [tag.get('href') for tag in soups]\n",
    "            for j in range(len(href_links)):\n",
    "                if '/en/documents/product-information' in str(href_links[j]):\n",
    "                    product_info_url_list.append(str(href_links[j]))\n",
    "            success = True\n",
    "        else:\n",
    "            print('error')\n",
    "            time.sleep(15)\n",
    "\n",
    "  \n"
   ],
   "id": "fc09b5cc74c3171c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.makedirs('pdf_files', exist_ok=True)\n",
    "\n",
    "# Iterate through the list of URLs\n",
    "for i, url in enumerate(product_info_url_list[1298:], start=1298):\n",
    "        # Send a GET request to the URL\n",
    "    success = False\n",
    "    while not success:\n",
    "        response = requests.get('https://www.ema.europa.eu'+ url)\n",
    "    \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "                # Define the PDF file name\n",
    "            pdf_file_name = os.path.join('pdf_files', f'{str(product_info_url_list[i]).replace(\"/\",\"-\")}')\n",
    "    \n",
    "                # Write the content to a PDF file\n",
    "            with open(pdf_file_name, 'wb') as pdf_file:\n",
    "                    pdf_file.write(response.content)\n",
    "    \n",
    "            print(f\"Downloaded: {pdf_file_name}\")\n",
    "            success = True\n",
    "        else:\n",
    "            print('error')\n",
    "            time.sleep(15)"
   ],
   "id": "a9f4e7bbca070e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "keywords = ['children','pediatric','pregnant','childbearing','breastfeeding']\n",
    "useful_info = []\n",
    "files = os.listdir('pdf_files')\n",
    "\n",
    "# Filter out only the PDF files\n",
    "pdf_files = [f for f in files if f.endswith('.pdf')]\n",
    "i=0\n",
    "# Iterate through each PDF file\n",
    "for pdf_file in pdf_files:\n",
    "    print(i)\n",
    "    useful_info_row = []\n",
    "    pdf_path = os.path.join('pdf_files', pdf_file)\n",
    "    useful_info_row.append(pdf_path.split('-')[5])\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            pdf_text = []\n",
    "            for page in pdf.pages:\n",
    "                pdf_text.append(page.extract_text())\n",
    "            for j in range(len(pdf_text)):\n",
    "                text = pdf_text[j]\n",
    "                sentence_list = text.split('.')\n",
    "                for k in range(len(sentence_list)):\n",
    "                    if any(keyword in sentence_list[k] for keyword in keywords):\n",
    "                        sentence_clean = sentence_list[k].replace('\\n','').lstrip()\n",
    "                        useful_info_row.append(sentence_clean)\n",
    "        useful_info.append(useful_info_row)\n",
    "        i=i+1\n",
    "    except Exception:\n",
    "        i=i+1\n",
    "        continue"
   ],
   "id": "ff9be4a76ae91ae5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_info_list = []\n",
    "for i in range(len(useful_info)):\n",
    "    lst = useful_info[i]\n",
    "    new_list = [lst[0],lst[1:]]\n",
    "    final_info_list.append(new_list)\n",
    "\n",
    "print(len(final_info_list))"
   ],
   "id": "7a84d33a356da4ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "final_df = pd.DataFrame(final_info_list, columns = ['drug name', 'relevant info'])\n",
    "print(final_df)\n",
    "final_df.to_csv('pediatric and pregnancy info for EMA drugs.csv')"
   ],
   "id": "7b9e9cde0bcd8019",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
